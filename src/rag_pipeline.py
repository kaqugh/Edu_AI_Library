import os
import sqlite3
import json
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from src.semantic_filters import infer_metadata_from_query

# === CONFIGURATION ===
DATA_PATH = Path.home() / "Edu_AI_Library" / "data"
VECTOR_PATH = Path.home() / "Edu_AI_Library" / "vectors" / "faiss_index"
CACHE_PATH = Path.home() / "Edu_AI_Library" / ".cache"
CACHE_PATH.mkdir(parents=True, exist_ok=True)

# === SEMANTIC CACHE SETUP ===
DB_PATH = CACHE_PATH / "answers.db"
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()
cursor.execute(
    """CREATE TABLE IF NOT EXISTS cache (
        query TEXT PRIMARY KEY,
        answer TEXT
    )"""
)
conn.commit()
print(f"‚ö° Semantic cache: SQLite at {DB_PATH}")

# === LOAD EMBEDDINGS + VECTOR STORE ===
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
db = FAISS.load_local(str(VECTOR_PATH), embeddings, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_kwargs={"k": 5})

print("RAG pipeline ready. Example calls:\n")
print("‚Ä¢ run_rag('List Arabic history books for grade 10')")
print("‚Ä¢ run_rag('What subjects does student U015 borrow most?', {'source':'users'})")


# === CACHE HANDLER ===
def get_cached_answer(query):
    cursor.execute("SELECT answer FROM cache WHERE query=?", (query,))
    result = cursor.fetchone()
    return json.loads(result[0]) if result else None


def store_cached_answer(query, answer):
    cursor.execute(
        "INSERT OR REPLACE INTO cache (query, answer) VALUES (?, ?)",
        (query, json.dumps(answer)),
    )
    conn.commit()


# === MAIN RAG FUNCTION ===
def run_rag(query: str, filters: dict = None, use_cache: bool = True):
    print(f"\nüîé Query: {query}")

    # 1Ô∏è‚É£ Check cache
    if use_cache:
        cached = get_cached_answer(query)
        if cached:
            print("üíæ Cache hit (SQLite)")
            return cached

    # 2Ô∏è‚É£ Infer semantic filters if none provided
    if filters is None:
        inferred = infer_metadata_from_query(query)
        print(f"üß† Inferred filters: {inferred}")
        filters = inferred
    else:
        print(f"üìò Using provided filters: {filters}")

    # 3Ô∏è‚É£ Retrieve initial candidate docs
    docs = retriever.invoke(query)
    if not docs:
        print("‚ö†Ô∏è No documents found by retriever.")
        return []

    # 4Ô∏è‚É£ Apply metadata-based filtering
    filtered_docs = []
    for d in docs:
        meta = d.metadata
        if all(
            str(meta.get(k, "")).lower() == str(v).lower()
            for k, v in filters.items()
            if k in meta
        ):
            filtered_docs.append(d)

    if not filtered_docs:
        print("‚ö†Ô∏è No relevant items found after applying semantic metadata filters.")
        print("üí° Try adjusting filters (subject/language/year) or updating dataset.")
        return []

    print(f"‚úÖ Retrieved {len(filtered_docs)} relevant items after filtering.")

    # 5Ô∏è‚É£ Prepare final response (basic summary)
    results = []
    for d in filtered_docs:
        snippet = d.page_content[:200].replace("\n", " ")
        results.append({
            "title": d.metadata.get("title", "Unknown"),
            "subject": d.metadata.get("subject", "N/A"),
            "language": d.metadata.get("language", "N/A"),
            "text": snippet
        })

    # 6Ô∏è‚É£ Store in semantic cache
    if use_cache:
        store_cached_answer(query, results)

    return results


# === AGENTIC DEMO ===
if __name__ == "__main__":
    print("\nRunning sample semantic query...\n")
    results = run_rag("List Arabic history books for grade 10")
    for r in results:
        print(f"üìò {r['title']} ({r['language']}) ‚Äî {r['subject']}")

